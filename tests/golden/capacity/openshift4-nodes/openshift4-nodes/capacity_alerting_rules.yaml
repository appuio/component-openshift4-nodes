apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  annotations:
    syn_component: openshift4-nodes
  labels:
    name: openshift4-nodes-capacity
  name: openshift4-nodes-capacity
  namespace: foo
spec:
  groups:
    - name: syn-NodesCpuCapacity
      rules:
        - alert: SYN_ClusterCpuUsageHigh
          annotations:
            description: The cluster is close to using up all CPU resources. The cluster
              might not be able to handle node failures or load spikes. Consider adding
              new nodes.
            message: Only {{ $value }} idle cpu cores accross cluster.
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/cpucapacity.html#SYN_ClusterCpuUsageHigh
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (label_replace(rate(node_cpu_seconds_total{mode="idle"}[15m]),
            "node", "$1", "instance", "(.+)") * on(node) group_left kube_node_role{role=~"(app)"}
            * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
            < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="cpu"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 30m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
        - alert: SYN_ExpectClusterCpuUsageHigh
          annotations:
            description: The cluster is getting close to using up all CPU resources.
              The cluster might soon not be able to handle node failures or load spikes.
              Consider adding new nodes.
            message: Cluster expected to run low on available CPU resources in 3 days
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/cpucapacity.html#SYN_ExpectClusterCpuUsageHigh
            syn_component: openshift4-nodes
          expr: predict_linear(avg_over_time(sum by (role, label_appuio_io_node_class)
            (label_replace(rate(node_cpu_seconds_total{mode="idle"}[15m]), "node",
            "$1", "instance", "(.+)") * on(node) group_left kube_node_role{role=~"(app)"}
            * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)[1d:5m])[1d:5m],
            3*24*60*60) < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="cpu"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 3h
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
    - name: syn-NodesMemoryCapacity
      rules:
        - alert: SYN_ClusterLowOnMemory
          annotations:
            description: The cluster is close to using all of its memory. The cluster
              might not be able to handle node failures or load spikes. Consider adding
              new nodes.
            message: Only {{ $value }} free memory on Worker Nodes.
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/memorycapacity.html#SYN_ClusterMemoryUsageHigh
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (label_replace(node_memory_MemAvailable_bytes,
            "node", "$1", "instance", "(.+)") * on(node) group_left kube_node_role{role=~"(app)"}
            * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
            < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="memory"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 30m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
        - alert: SYN_ExpectClusterLowOnMemory
          annotations:
            description: The cluster is getting close to using all of its memory.
              Soon the cluster might not be able to handle node failures or load spikes.
              Consider adding new nodes.
            message: Cluster expected to run low on memory in 3 days
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/memorycapacity.html#SYN_ExpectClusterMemoryUsageHigh
            syn_component: openshift4-nodes
          expr: predict_linear(avg_over_time(sum by (role, label_appuio_io_node_class)
            (label_replace(node_memory_MemAvailable_bytes, "node", "$1", "instance",
            "(.+)") * on(node) group_left kube_node_role{role=~"(app)"} * on(node)
            group_left(role, label_appuio_io_node_class) kube_node_labels)[1d:5m])[1d:5m],
            3*24*60*60) < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="memory"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 3h
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
    - name: syn-NodesPodCapacity
      rules:
        - alert: SYN_ExpectTooManyPods
          annotations:
            description: The cluster is getting close to the limit of running pods.
              Soon the cluster might not be able to handle node failures and might
              not be able to start new pods. Consider adding new nodes.
            message: Expected to exceed the threshold of running pods in 3 days
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/podcapacity.html#SYN_ExpectTooManyPods
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (kube_node_status_capacity{resource="pods"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels) - predict_linear(avg_over_time(sum
            by (role, label_appuio_io_node_class) (kubelet_running_pods * on(node)
            group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class)
            kube_node_labels)[1d:5m])[1d:5m], 3*24*60*60) < 1.000000 * max by (role,
            label_appuio_io_node_class) ((kube_node_status_capacity{resource="pods"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 3h
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
        - alert: SYN_TooManyPods
          annotations:
            description: The cluster is close to the limit of running pods. The cluster
              might not be able to handle node failures and might not be able to start
              new pods. Consider adding new nodes.
            message: Only {{ $value }} more pods can be started.
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/podcapacity.html#SYN_TooManyPods
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (kube_node_status_capacity{resource="pods"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels) - sum by (role, label_appuio_io_node_class)
            (kubelet_running_pods * on(node) group_left kube_node_role{role=~"(app)"}
            * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
            < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="pods"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 30m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
    - name: syn-NodesResourceRequests
      rules:
        - alert: SYN_ExpectTooMuchCPURequested
          annotations:
            description: The cluster is getting close to assigning all CPU cores to
              running pods. Soon the cluster might not be able to handle node failures
              and might not be able to start new pods. Consider adding new nodes.
            message: Expected to exceed the threshold of requested CPU resources in
              3 days
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_ExpectTooMuchCPURequested
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (kube_node_status_allocatable{resource="cpu"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels) - predict_linear(avg_over_time(sum
            by (role, label_appuio_io_node_class) (kube_pod_resource_request{resource="cpu"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)[1d:5m])[1d:5m], 3*24*60*60)
            < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_allocatable{resource="cpu"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 3h
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
        - alert: SYN_ExpectTooMuchMemoryRequested
          annotations:
            description: The cluster is getting close to assigning all memory to running
              pods. Soon the cluster might not be able to handle node failures and
              might not be able to start new pods. Consider adding new nodes.
            message: Expected to exceed the threshold of requested memory in 3 days
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_ExpectTooMuchMemoryRequested
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (kube_node_status_allocatable{resource="memory"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels) - predict_linear(avg_over_time(sum
            by (role, label_appuio_io_node_class) (kube_pod_resource_request{resource="memory"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)[1d:5m])[1d:5m], 3*24*60*60)
            < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_allocatable{resource="memory"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 3h
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
        - alert: SYN_TooMuchCPURequested
          annotations:
            description: The cluster is close to assigning all CPU resources to running
              pods. The cluster might not be able to handle node failures and might
              soon not be able to start new pods. Consider adding new nodes.
            message: Only {{ $value }} cpu cores left for new pods.
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_TooMuchCPURequested
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (kube_node_status_allocatable{resource="cpu"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels) - sum by (role, label_appuio_io_node_class)
            (kube_pod_resource_request{resource="cpu"} * on(node) group_left kube_node_role{role=~"(app)"}
            * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
            < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_allocatable{resource="cpu"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 30m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
        - alert: SYN_TooMuchMemoryRequested
          annotations:
            description: The cluster is close to assigning all memory to running pods.
              The cluster might not be able to handle node failures and might not
              be able to start new pods. Consider adding new nodes.
            message: Only {{ $value }} memory left for new pods.
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/resourcerequests.html#SYN_TooMuchMemoryRequested
            syn_component: openshift4-nodes
          expr: sum by (role, label_appuio_io_node_class) (kube_node_status_allocatable{resource="memory"}
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels) - sum by (role, label_appuio_io_node_class)
            (kube_pod_resource_request{resource="memory"} * on(node) group_left kube_node_role{role=~"(app)"}
            * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
            < 1.000000 * max by (role, label_appuio_io_node_class) ((kube_node_status_allocatable{resource="memory"})
            * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role,
            label_appuio_io_node_class) kube_node_labels)
          for: 30m
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
    - name: syn-NodesUnusedCapacity
      rules:
        - alert: SYN_ClusterHasUnusedNodes
          annotations:
            description: The cluster has {{ $value }} unused nodes. Consider removing
              unused nodes.
            message: Cluster has unused nodes.
            runbook_url: https://hub.syn.tools/openshift4-monitoring/runbooks/unusedcapacity.html#SYN_ClusterHasUnusedNodes
            syn_component: openshift4-nodes
          expr: |-
            min by (role, label_appuio_io_node_class) ((
              label_replace(
                (sum by (role, label_appuio_io_node_class) (kube_node_status_capacity{resource="pods"} * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels) - sum by (role, label_appuio_io_node_class) (kubelet_running_pods * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)) / max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="pods"}) * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
              , "resource", "pods", "", "")
            ) or (
              label_replace(
                (sum by (role, label_appuio_io_node_class) (kube_node_status_allocatable{resource="memory"} * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels) - sum by (role, label_appuio_io_node_class) (kube_pod_resource_request{resource="memory"} * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)) / max by (role, label_appuio_io_node_class) ((kube_node_status_allocatable{resource="memory"}) * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
              , "resource", "requested_memory", "", "")
            ) or (
              label_replace(
                (sum by (role, label_appuio_io_node_class) (kube_node_status_allocatable{resource="cpu"} * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels) - sum by (role, label_appuio_io_node_class) (kube_pod_resource_request{resource="cpu"} * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)) / max by (role, label_appuio_io_node_class) ((kube_node_status_allocatable{resource="cpu"}) * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
              , "resource", "requested_cpu", "", "")
            ) or (
              label_replace(
                sum by (role, label_appuio_io_node_class) (label_replace(node_memory_MemAvailable_bytes, "node", "$1", "instance", "(.+)") * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels) / max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="memory"}) * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
              , "resource", "memory", "", "")
            ) or (
              label_replace(
                sum by (role, label_appuio_io_node_class) (label_replace(rate(node_cpu_seconds_total{mode="idle"}[15m]), "node", "$1", "instance", "(.+)") * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels) / max by (role, label_appuio_io_node_class) ((kube_node_status_capacity{resource="cpu"}) * on(node) group_left kube_node_role{role=~"(app)"} * on(node) group_left(role, label_appuio_io_node_class) kube_node_labels)
              , "resource", "cpu", "", "")
            )
            ) > 4.000000
          for: 8h
          labels:
            severity: warning
            syn: 'true'
            syn_component: openshift4-monitoring
